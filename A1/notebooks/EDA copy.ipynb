{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background knowledge:\n",
    "The dataset contains multiple activities() in time sequence </br>\n",
    "Type of activity:\n",
    "- 1=walking; \n",
    "- 2=descending stairs; \n",
    "- 3=ascending stairs; \n",
    "- 4=driving;\n",
    "- 77=clapping; \n",
    "- 99=non-study activity\n",
    "</br>\n",
    "The **goal** of the project is to use Nerual networks (deep learning) methods to distinguish which activity is which.</br>\n",
    "More info refer to file [raw_accelerometry_data_dict.csv](A1/data/raw/physionet.org/files/accelerometry-walk-climb-drive/1.0.0/raw_accelerometry_data_dict.csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow of preprocessing:\n",
    "- 1:load the dataset\n",
    "- 2:seperate the activity and plot each divices on different body part\n",
    "- 3:standardize and normalize data\n",
    "- 4:use sliding window and segementation\n",
    "- 5:calculate Vector magnitude func:calculate_vector_magnitude\n",
    "- 6:use noise reduction\n",
    "- 7:use FFT \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- make train / test dataset\n",
    "- make CNN / LSTM etc models\n",
    "- build training pipeline\n",
    "- evaluation and compare \n",
    "- find the results\n",
    "\n",
    "\n",
    "https://github.com/markdregan/K-Nearest-Neighbors-with-Dynamic-Time-Warping/blob/master/K_Nearest_Neighbor_Dynamic_Time_Warping.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import matplotlib\n",
    "import calendar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import medfilt, butter, filtfilt\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/diegozanutti/HumanDataAnalytics/A1\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -c --timeout 10 https://physionet.org/static/published-projects/accelerometry-walk-climb-drive/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip -O dataset.zip\n",
    "!unzip -d dataset -q dataset.zip\n",
    "!echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id00b70b13.csv id3e3e50c7.csv id82b9735c.csv idb221f542.csv idf540d82b.csv\n",
      "id079c763c.csv id4ea159a8.csv id86237981.csv idbae5a811.csv idf5e3678b.csv\n",
      "id1165e00c.csv id5308a7d6.csv id8af5374b.csv idc735fc09.csv idfc5f05e4.csv\n",
      "id1c7e64ad.csv id5993bf4a.csv id8e66893c.csv idc91a49d0.csv idff99de96.csv\n",
      "id1f372081.csv id650857ca.csv id9603e9c3.csv idd80ac2b4.csv\n",
      "id34e056c8.csv id687ab496.csv ida61e8ddf.csv idecc9265e.csv\n",
      "id37a54bbf.csv id7c20ee7a.csv idabd0c53c.csv idf1ce9a0f.csv\n"
     ]
    }
   ],
   "source": [
    "!ls src/dataset/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_single_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df   \n",
    "    # select by activity and plot them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_all_data(folder_path):\n",
    "    # List to store the DataFrames\n",
    "    data_frames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path,filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            data_frames.append(df)\n",
    "        else:\n",
    "            print(f\"{filename} is not a csv file\")\n",
    "        \n",
    "    return data_frames \n",
    "    # print(df.head(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_all(df,path):\n",
    "    sns.set_theme(context='notebook', style='darkgrid', palette=\"deep\")\n",
    "    # variables = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "\n",
    "    variables = {\n",
    "    'magnitude_0': 'lw',\n",
    "    'magnitude_1': 'lh',\n",
    "    'magnitude_2': 'la',\n",
    "    'magnitude_3': 'ra'\n",
    "}\n",
    "   \n",
    "    # df.plot(x='time_s', y=variables)\n",
    "    df.plot(x='time_s', y=list(variables.keys()))\n",
    "    # tick_positions = range(0, 3034, 100) \n",
    "    # plt.xticks(tick_positions)\n",
    "    # Adding labels and a legend\n",
    "    \n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend([variables[var] for var in variables.keys()])\n",
    "    \n",
    "    # Displaying the plot\n",
    "    # plt.show()\n",
    "    plt.savefig(path)\n",
    "\n",
    "# plot(combined_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_x_y_z_plot(df):\n",
    "    # colors = [\"#F7F3E3\",\"#7F7CAF\",\"#78C0E0\"]\n",
    "    option_colors = sns.color_palette()\n",
    "    colors =[option_colors[0],option_colors[1],option_colors[2]]\n",
    "    # create larger subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        # Plotting\n",
    "    for i, (ax, variables) in enumerate(zip(axs.flat, [('lw_x', 'lw_y', 'lw_z'), ('lh_x', 'lh_y', 'lh_z'), ('la_x', 'la_y', 'la_z'), ('ra_x', 'ra_y', 'ra_z')])):\n",
    "        for variable, color in zip(variables, colors):\n",
    "            sns.lineplot(data=df, x='time_s', y=variable, ax=ax, linewidth=0.5, color=color, label=variable)\n",
    "        \n",
    "        ax.legend(loc='upper right', fontsize=12)\n",
    "        ax.set_title(' vs time_s '.join(variables), fontsize=16)\n",
    "        ax.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax.set_ylabel('Value', fontsize=14)\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seperate activity for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate each activity based on the numebr\n",
    "def separate_activity(df,activity):\n",
    "    if activity == 99:\n",
    "        df_non_study = df[df['activity'] == 99]\n",
    "        return df_non_study\n",
    "    elif activity == 77:\n",
    "        df_clapping = df[df['activity'] == 77]\n",
    "        return df_clapping\n",
    "    elif activity ==4:\n",
    "        df_driving = df[df['activity'] == 4]\n",
    "        return df_driving\n",
    "    elif activity ==3:\n",
    "        df_ascending_stairs = df[df['activity'] == 3]\n",
    "        return df_ascending_stairs\n",
    "    elif activity ==2:\n",
    "        df_descending_stairs = df[df['activity'] == 2]\n",
    "        return df_descending_stairs\n",
    "    elif activity ==1:\n",
    "        df_walking = df[df['activity'] == 1]\n",
    "        return df_walking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the period of each activity\n",
    "def show_x_y_z_plot(df,activity):\n",
    "    if activity == 99:\n",
    "        df_non_study = df[df['activity'] == 99]\n",
    "        # df_99 = df_99[(df_99['time_s'] >= 2600) & (df_99['time_s'] <= 2800)]\n",
    "        make_x_y_z_plot(df_99)\n",
    "    elif activity == 77:\n",
    "        df_77 = df[df['activity'] == 77]\n",
    "        # df_77 = df_77[(df_77['time_s'] >= 520) & (df_77['time_s'] <= 540)]\n",
    "        make_x_y_z_plot(df_77)\n",
    "    elif activity ==4:\n",
    "        df_4 = df[df['activity'] == 4]\n",
    "        # df_4 = df_4[(df_4['time_s'] >= 1400) & (df_4['time_s'] <= 1600)]\n",
    "        make_x_y_z_plot(df_4)\n",
    "    elif activity ==3:\n",
    "        df_3 = df[df['activity'] == 3]\n",
    "        # df_3 = df_3[(df_3['time_s'] >= 300) & (df_3['time_s'] <= 350)]\n",
    "        make_x_y_z_plot(df_3)\n",
    "    elif activity ==2:\n",
    "        df_2 = df[df['activity'] == 2]\n",
    "        # df_2 = df_2[(df_2['time_s'] >= 300) & (df_2['time_s'] <= 350)]\n",
    "        make_x_y_z_plot(df_2)\n",
    "    elif activity ==1:\n",
    "        df_1 = df[df['activity'] == 1]\n",
    "        # df_1 = df_1[(df_1['time_s'] >= 650) & (df_1['time_s'] <= 750)] \n",
    "        make_x_y_z_plot(df_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(df):\n",
    "    # initialize standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    # select the columns to standardize\n",
    "    # columns_to_standardize = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    \n",
    "    columns_to_standardize = ['magnitude_0','magnitude_1', 'magnitude_2', 'magnitude_3']\n",
    "\n",
    "    # standardize the columns\n",
    "    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "    # Convert the 'time_s' column to a datetime format if it isn't already\n",
    "    df['time_s'] = pd.to_datetime(df['time_s'], unit='s') \n",
    "    # Set the 'time_s' column as the index of the DataFrame\n",
    "    df.set_index('time_s', inplace=True)\n",
    "        # check if the data has been standardized\n",
    "    # print(df[columns_to_standardize].mean())  # should be close to 0\n",
    "    # print(df[columns_to_standardize].std())   # should be close to 1\n",
    "\n",
    "    # Reset the index back to RangeIndex\n",
    "    df.reset_index(inplace=True)\n",
    "    # Convert 'time_s' back to its original unit\n",
    "    df['time_s'] = df['time_s'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=3):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(df, columns, cutoff, fs, order=3):\n",
    "    df_filtered = df.copy()\n",
    "    for column in columns:\n",
    "        b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "        df_filtered[column] = filtfilt(b, a, df[column].values)\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_noise_filter(df):\n",
    "    # Assuming you want to apply a median filter and low-pass Butterworth filter\n",
    "    # columns_to_apply_filter = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    columns_to_apply_filter = ['magnitude_0','magnitude_1', 'magnitude_2', 'magnitude_3']\n",
    "\n",
    "    # Apply median filter to the specified columns\n",
    "    df_filtered = df.copy()\n",
    "    for column in columns_to_apply_filter:\n",
    "        df_filtered[column] = medfilt(df[column], kernel_size=5)  # Adjust kernel size as needed\n",
    "\n",
    "    # Apply low-pass Butterworth filter to the filtered data\n",
    "    cutoff_freq = 0.2  # Corner frequency in Hz\n",
    "    order = 3  # Butterworth filter order\n",
    "    fs = 100  # Sample rate (assuming equally spaced samples)\n",
    "    df_filtered = butter_lowpass_filter(df_filtered, columns_to_apply_filter, cutoff_freq, fs, order)\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standardize and segment\n",
    "def data_processed(df):\n",
    "    \n",
    "    df_new = standardize(df)\n",
    "    # Assuming your preprocessed data is stored in the DataFrame 'df_preprocessed'\n",
    "    df_filtered = apply_noise_filter(df_new)\n",
    "\n",
    "    # window_size = 3  # Size of each window in seconds\n",
    "    window_size = [5.12,10.24]\n",
    "    overlap = 0.5  # Overlap percentage (50%)\n",
    "    sampling_rate = 100  # Sampling rate of your data (samples per second)\n",
    "    #(i.e. 2.56s Ã— 100Hz *0.5 =  128 sample ref: paper1\n",
    "\n",
    "    # Calculate the number of data points in each window\n",
    "    window_length = int(window_size[0] * sampling_rate)\n",
    "\n",
    "    # Calculate the number of data points to shift the window by for the given overlap\n",
    "    shift_length = int(window_length * overlap)\n",
    "\n",
    "    # Initialize an empty list to store the segmented data\n",
    "    segmented_data = []\n",
    "\n",
    "    # Iterate over the data using a sliding window\n",
    "    start_index = 0\n",
    "    while start_index + window_length <= len(df_filtered):\n",
    "        end_index = start_index + window_length\n",
    "        segment = df_filtered.iloc[start_index:end_index]\n",
    "        segmented_data.append(segment)\n",
    "        start_index += shift_length\n",
    "\n",
    "    # Concatenate the segmented data into a new DataFrame\n",
    "    df_segmented = pd.concat(segmented_data)\n",
    "\n",
    "    # Reset the index of the segmented DataFrame\n",
    "    df_segmented.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_vector_magnitude(df):\n",
    "    df_transformed = df.copy()\n",
    "    columns_to_calculate_VM = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    for i in range(0,len(columns_to_calculate_VM),3):\n",
    "        x_col = columns_to_calculate_VM[i]\n",
    "        y_col = columns_to_calculate_VM[i+1]\n",
    "        z_col = columns_to_calculate_VM[i+2]\n",
    "        magnitude_col = f\"magnitude_{i//3}\"\n",
    "        # Calculate vector magnitude while removing sensor orientation\n",
    "        df_transformed[magnitude_col] = np.sqrt(df[x_col]**2 + df[y_col]**2 + df[z_col]**2)\n",
    "    # print(df_transformed.columns)\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def statistical_extraction(data_processed,activity_name):\n",
    "    results = []\n",
    "    # calculate mean,std,variance,minimum,maximun,\n",
    "    mean = np.mean(data_processed)\n",
    "     # Calculate standard deviation\n",
    "    std = np.std(data_processed)\n",
    "\n",
    "    # Calculate variance\n",
    "    variance = np.var(data_processed)\n",
    "\n",
    "    # Calculate minimum\n",
    "    minimum = np.min(data_processed)\n",
    "\n",
    "    # Calculate maximum\n",
    "    maximum = np.max(data_processed)\n",
    "    result = {\n",
    "            'activity': f'{activity_name}',\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'variance': variance,\n",
    "            'minimum': minimum,\n",
    "            'maximum': maximum\n",
    "        }\n",
    "        \n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "variables = {\n",
    "    'magnitude_0': 'lw',\n",
    "    'magnitude_1': 'lh',\n",
    "    'magnitude_2': 'la',\n",
    "    'magnitude_3': 'ra'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FFT_plot(fft_results, labels):\n",
    "    N = len(fft_results[0])\n",
    "    sampling_rate = 100  # Assuming a sampling rate of 100 Hz\n",
    "    frequencies = np.fft.fftfreq(N, d=1/sampling_rate)\n",
    "    \n",
    "    # Plot the FFT results\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.set_theme(context='notebook', style='darkgrid', palette=\"deep\")\n",
    "    \n",
    "    for fft_result, label in zip(fft_results, labels):\n",
    "        ax.plot(frequencies, fft_result, label=label)\n",
    "    \n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_ylabel('Magnitude')\n",
    "    ax.set_title('FFT Results')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fft_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m         fft_results\u001b[38;5;241m.\u001b[39mappend(fft_result)\n\u001b[1;32m      9\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[0;32m---> 11\u001b[0m FFT_plot(\u001b[43mfft_results\u001b[49m, labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fft_results' is not defined"
     ]
    }
   ],
   "source": [
    "def FFT_DWT(VM, variables):\n",
    "    fft_results = []\n",
    "    labels = []\n",
    "    for key, label in variables.items():\n",
    "        X = fft(VM[key].values)\n",
    "        fft_result = np.abs(X)\n",
    "        print(f'FFT result for {label}:', fft_result)\n",
    "        fft_results.append(fft_result)\n",
    "        labels.append(label)\n",
    "    \n",
    "FFT_plot(fft_results, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VMC():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_demographics():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_dict = {   \n",
    "        \"non-study activity\" : 99,\n",
    "        \"clapping\" : 77,\n",
    "        \"driving\" : 4,\n",
    "        \"ascending stairs\" : 3,\n",
    "        \"descending stairs\" : 2,\n",
    "        \"walking\": 1\n",
    "    }\n",
    "    # Folder path containing the CSV files\n",
    "folder_path = '../data/raw/physionet.org/files/accelerometry-walk-climb-drive/1.0.0/raw_accelerometry_data'\n",
    "file_path = '../data/raw/physionet.org/files/accelerometry-walk-climb-drive/1.0.0/raw_accelerometry_data/id00b70b13.csv'\n",
    "file_path2 = '../data/raw/physionet.org/files/accelerometry-walk-climb-drive/1.0.0/raw_accelerometry_data/id1c7e64ad.csv'\n",
    "plt.rcParams['figure.figsize'] = (20, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_can_do_all(file_path,activity):\n",
    "     # Generate the save path\n",
    "    results_before = '../output/transform/before noise filter/output'\n",
    "    save_path_before = f\"{results_before}_{activity}.jpg\"\n",
    "\n",
    "    results_after = '../output/transform/after noise filter/output'\n",
    "    save_path_after = f\"{results_after}_{activity}.jpg\"\n",
    "\n",
    "    os.makedirs(results_before,exist_ok=True)\n",
    "    os.makedirs(results_after,exist_ok=True)\n",
    "    \n",
    "    # 1.read and plot\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_activity = separate_activity(df,activity)\n",
    "    df_activity_tranform = calculate_vector_magnitude(df_activity)\n",
    "    # print(df_activity_tranform)\n",
    "#     plot_all(df_activity_tranform,save_path_before)\n",
    "    \n",
    "\n",
    "    # 2.standardize and downsampling and segmentation\n",
    "    df_activity_processed = data_processed(df_activity_tranform)\n",
    "   \n",
    "    # 3. Noise filtering\n",
    "    filtered_data = apply_noise_filter(df_activity_processed)\n",
    "    \n",
    "    # 4.statical analysis\n",
    "    results = statistical_extraction(filtered_data,activity)\n",
    "\n",
    "    plot_all(filtered_data,save_path_after)\n",
    "    # FFT_DWT(filtered_data,variables)\n",
    "    # FFT_DWT(df_activity_tranform,variables)\n",
    "    \n",
    "   \n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    results = []\n",
    "    # activity_name = 'walking'\n",
    "    for activity in activity_dict:\n",
    "        print(f\"Now activity is {activity} and the code is {activity_dict[activity]}\")    \n",
    "        activity_results = method_can_do_all(file_path2,activity_dict[activity])\n",
    "        # print(activity_results)\n",
    "\n",
    "        # results.extend(activity_results)\n",
    "    # Convert results to a DataFrame\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_df.to_csv('statistical_resultsid.id1c7e64ad.csv', index=False)\n",
    "\n",
    "    # Display results as a table\n",
    "    # print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement ARIMA for predicting furture trend or prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement KNN "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
