{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5497d60-cc1e-495a-8ec0-bd436797bb73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (utils.py, line 112)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3433\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 11\u001b[0;36m\n\u001b[0;31m    from utils.utils import select_model,train_model,plot\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/HumanDataAnalytics/A1/src/utils/utils.py:112\u001b[0;36m\u001b[0m\n\u001b[0;31m    model.save('DZ_CNN.h5')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "from data.data_segmentation import DataSegmentation\n",
    "from data.data_loader import DataLoader\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from utils.activity_type import ActivityType\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import datetime\n",
    "import os\n",
    "from utils.utils import select_model,train_model,plot\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "import logging\n",
    "# System and File Operations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from utils.utils import load_person_df_map, preprocess_data\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607e0f6-f992-4fe6-a355-c16f77c63d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "def reset_seeds():\n",
    "   os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "   np.random.seed(42) \n",
    "   rn.seed(12345)\n",
    "   tf.random.set_seed(1234)\n",
    "\n",
    "\n",
    "def preprocess_method_1():\n",
    "    # First method of preprocessing\n",
    "    data_loader = DataLoader(\"dataset/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data\")\n",
    "    data_loader.download_data()\n",
    "    data_loader.read_files()\n",
    "    \n",
    "   \n",
    "    data_seg = DataSegmentation(window_duration=1.28, overlap=0.5, sampling_rate=100)\n",
    "\n",
    "\n",
    "    train_data_X,train_data_y = data_seg(data_loader.train_data)\n",
    "    test_data_X,test_data_y = data_seg(data_loader.test_data)\n",
    "\n",
    "    label_mapping = ActivityType.create_label_mapping()\n",
    "    \n",
    "\n",
    "    # one_hot_encoded_train_y = ActivityType.one_hot(train_data_y, label_mapping)\n",
    "    # one_hot_encoded_test_y = ActivityType.one_hot(test_data_y, label_mapping)\n",
    "\n",
    "    \n",
    "    # final_train_y = one_hot_encoded_train_y.reshape(one_hot_encoded_train_y.shape[0],-1)\n",
    "    # final_test_y = one_hot_encoded_test_y.reshape(one_hot_encoded_test_y.shape[0],-1)\n",
    "\n",
    "    \n",
    "    train_data_y_1d = np.squeeze(train_data_y)\n",
    "    test_data_y_1d = np.squeeze(test_data_y)\n",
    "\n",
    "    train_data_y_1d_mapped = np.vectorize(label_mapping.get)(train_data_y_1d)\n",
    "    test_data_y_1d_mapped = np.vectorize(label_mapping.get)(test_data_y_1d)\n",
    "\n",
    "    return train_data_X,train_data_y_1d_mapped,test_data_X,test_data_y_1d_mapped\n",
    "\n",
    "def preprocess_method_2():\n",
    "    WALKING = 1\n",
    "    DESCENDING = 2\n",
    "    ASCENDING = 3\n",
    "    activities_list_to_consider = [WALKING, DESCENDING, ASCENDING]\n",
    "    person_df_map = load_person_df_map(activities_list_to_consider)\n",
    "    X, y = preprocess_data(person_df_map)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "\n",
    "    # Reshape input for Conv1D\n",
    "    num_features = X_train.shape[2]\n",
    "    window_size=128\n",
    "    X_train = X_train.reshape((-1, window_size, num_features))\n",
    "    X_test = X_test.reshape((-1, window_size, num_features))\n",
    "    return X_train, y_train,X_test,y_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912472cd-67b6-4790-8f61-3d00c911cd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_magnitude(data):\n",
    "    \"\"\"\n",
    "    Calculate the magnitude of the features for a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array with shape (ntrials, samples, features)\n",
    "\n",
    "    Returns:\n",
    "    - magnitude_data: numpy array with shape (ntrials, samples, 4) representing the magnitude of each accelerometer\n",
    "    \"\"\"\n",
    "    # Assuming data has shape (ntrials, samples, 12)\n",
    "    num_accelerometers = 4\n",
    "    features_per_accelerometer = 3\n",
    "\n",
    "    # Reshape the data to separate X, Y, Z for each accelerometer\n",
    "    reshaped_magnitude = data.reshape(data.shape[0], data.shape[1], num_accelerometers, features_per_accelerometer)\n",
    "\n",
    "    # Calculate the magnitude for each accelerometer\n",
    "    magnitude = np.sqrt(np.sum(reshaped_magnitude**2, axis=-1))\n",
    "\n",
    "    displacement_vector = np.diff(data, axis=1)\n",
    "\n",
    "    # Pad the displacement vector with one sample of value 0 along axis 1\n",
    "    displacement_vector_padded = np.concatenate([np.zeros((displacement_vector.shape[0], 1, displacement_vector.shape[2])), displacement_vector], axis=1)\n",
    "\n",
    "\n",
    "    reshaped_displacement = displacement_vector_padded.reshape(data.shape[0], data.shape[1], num_accelerometers, features_per_accelerometer)\n",
    "    # Calculate the magnitude of the displacement vector\n",
    "    displacement_magnitude = np.sqrt(np.sum(reshaped_displacement**2, axis=-1))\n",
    "\n",
    "    return magnitude, displacement_magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74469c9-ebe8-42c3-9953-1a8440ab4fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    LABELS = [\n",
    "        \"Walking\",\n",
    "        \"Descending Stairs\",\n",
    "        \"Ascending Stairs\"\n",
    "    ]\n",
    "    epochs = 20\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.00005\n",
    "    num_runs = 1\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = f\"training_log_{current_time}.txt\"\n",
    "    logging.basicConfig(filename=filename, level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "\n",
    "    # Choose one of the two methods of preprocessing\n",
    "    method = 1\n",
    "    if method == 1:\n",
    "        train_data_X,train_data_y_1d_mapped,test_data_X,test_data_y_1d_mapped = preprocess_method_1()\n",
    "    else:\n",
    "        X_train,y_train,X_test,y_test = preprocess_method_2()\n",
    "\n",
    "    magnitude_result_train, displacement_result_train = calculate_magnitude(train_data_X)\n",
    "    train_data_X_full = np.concatenate([train_data_X, magnitude_result_train, displacement_result_train],axis=-1)\n",
    "    \n",
    "    magnitude_result_test, displacement_result_test = calculate_magnitude(test_data_X)\n",
    "    test_data_X_full = np.concatenate([test_data_X, magnitude_result_test, displacement_result_test],axis=-1)\n",
    "\n",
    "    models = [\"DZ_CNN\"] #\"LSTM_CNN\", \"Dual_LSTM\", \"DeepConvLSTM3\",\n",
    "    for model_name in models:\n",
    "        with open(filename, \"w\") as file:\n",
    "            logger.info(\n",
    "            \"Training Log\\n\"\n",
    "            f\"Date and Time: {datetime.datetime.now()}\\n\"\n",
    "            f\"Running the training process {num_runs} times\\n\\n\"\n",
    "            f\"Seed for this run is: {42}, {12345}, {1234}\\n\\n\"\n",
    "            f\"Training epoch: {epochs}, learning rate: {learning_rate}, \"\n",
    "            f\"batch_size = {batch_size}, model is: {model_name} with new way of segmenting data\\n\"\n",
    "            )\n",
    "            for i in range(num_runs):\n",
    "                tf.compat.v1.enable_eager_execution()\n",
    "                reset_seeds() \n",
    "                # tf.compat.v1.disable_eager_execution()  # Or enable, depending on your requirement\n",
    "                # loss,accuracy,precision,recall,f1= train_model(model_name,X_train,y_train,X_test,y_test) #first method\n",
    "                loss,accuracy,precision,recall,f1= train_model(model_name,train_data_X_full[:,:,12:],train_data_y_1d_mapped,test_data_X_full[:,:,12:],test_data_y_1d_mapped,batch_size, epochs)\n",
    "                # loss,accuracy,precision,recall,f1= train_model(model_name,X_train,y_train,X_test,y_test) #second method\n",
    "                logger.info(f\"Run {i+1}: \\n\"\n",
    "                            f\"Accuracy = {accuracy}\\n\"\n",
    "                            f\"Precision = {precision}\\n\"\n",
    "                            f\"Recall = {recall}\\n\"\n",
    "                            f\"F1 = {f1}\\n\"\n",
    "                            f\"Loss = {loss}\\n\")\n",
    "                tf.keras.backend.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a02539-7fe2-40b0-b94e-2b1da3247bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49c28b-799d-4d20-854e-998c72a4f063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
